{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from DataMatrix import generate_data_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data_matrix(method=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dbscan(data, min_samples, eps):\n",
    "\n",
    "    # Initializing labels to -1\n",
    "    labels = np.full(data.shape[0], -1, dtype=int)\n",
    "\n",
    "    # Initializing visited set\n",
    "    visited = set()\n",
    "\n",
    "    # Initializing core_dict to store core points and map to their neighborhoods\n",
    "    core_dict = {}\n",
    "\n",
    "    # Computing the distance matrix\n",
    "    distanceMatrix = pairwise_distances(data, metric='euclidean')\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        # Skipping if already visited\n",
    "        if i in visited:\n",
    "            continue\n",
    "\n",
    "        # Visiting the point\n",
    "        visited.add(i)\n",
    "\n",
    "        # Finding neighbors\n",
    "        neighbors = [j for j in range(data.shape[0]) if distanceMatrix[i][j] <= eps]\n",
    "\n",
    "        if len(neighbors) < min_samples: # If not a core point\n",
    "            labels[i] = -1\n",
    "        else: # If a core point\n",
    "\n",
    "            # Assigning label to core point\n",
    "            labels[i] = i\n",
    "\n",
    "            # Adding core point to core_dict\n",
    "            core_dict[i] = neighbors\n",
    "\n",
    "            for j in neighbors:\n",
    "                # Skipping if already visited\n",
    "                if j in visited:\n",
    "                    continue\n",
    "\n",
    "                # Finding neighbors of neighbors\n",
    "                neighbors2 = [k for k in range(data.shape[0]) if distanceMatrix[j][k] <= eps]\n",
    "\n",
    "                # Visiting the point\n",
    "                visited.add(j)\n",
    "\n",
    "                # Assigning label of the neighbor to the same one as core point\n",
    "                labels[j] = i\n",
    "\n",
    "                # Adding neighbors of neighbors to core_dict which will be assigned the same label as core point later in the last loop\n",
    "                if len(neighbors2) >= min_samples:\n",
    "                    core_dict[j] = neighbors2\n",
    "\n",
    "    # Assigning labels to non-core points based on their core point\n",
    "    for label,neighborhood in core_dict.items():\n",
    "        for neighbor in neighborhood:\n",
    "            if labels[neighbor] == -1:\n",
    "                labels[neighbor] = label\n",
    "\n",
    "    # Convert labels to integers\n",
    "    labels = labels.astype(int)\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Testing the DBSCAN algorithm\n",
    "labelsInNumbers = dbscan(x_train, 5, 0.5)\n",
    "\n",
    "# Print number of noise points (label = -1)\n",
    "print('Number of noise points: ', np.count_nonzero(labelsInNumbers == -1))\n",
    "\n",
    "# Create array of strings to store the final labels\n",
    "labelsInString = np.empty(labelsInNumbers.shape[0], dtype=object)\n",
    "\n",
    "# The number of clusters will be equal to the number of unique labels\n",
    "clusters = np.unique(labelsInNumbers)\n",
    "print('Number of clusters: ', clusters.shape[0])\n",
    "\n",
    "for i in range(clusters.shape[0]):\n",
    "\n",
    "    # Finding the indices of the points in the cluster\n",
    "    labels = np.where(labelsInNumbers == clusters[i])\n",
    "\n",
    "    # Creating a dictionary to count the number of each label in the cluster\n",
    "    counterLabels = {}\n",
    "    for label in y_train[labels]:\n",
    "        counterLabels[label] = counterLabels.get(label, 0) + 1\n",
    "\n",
    "    # Finding the most common label\n",
    "    maxLabel = max(counterLabels, key=counterLabels.get)\n",
    "\n",
    "    # Assigning the most common label to all the points in the cluster\n",
    "    labelsInString[labels] = maxLabel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred = labelsInString\n",
    "\n",
    "# Evaluating the algorithm\n",
    "print(\"Macro: \")\n",
    "print(\"Precision: \", precision_score(y_train, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_train, y_pred, average='macro'))\n",
    "print(\"F1 score: \", f1_score(y_train, y_pred, average='macro'))\n",
    "print(\"Accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Weighted: \")\n",
    "print(\"Precision: \", precision_score(y_train, y_pred, average='weighted'))\n",
    "print(\"Recall: \", recall_score(y_train, y_pred, average='weighted'))\n",
    "print(\"F1 score: \", f1_score(y_train, y_pred, average='weighted'))\n",
    "print(\"Accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "# Now, let's measure the conditional entropy of the clusters to see if they are well separated\n",
    "totalEntropy = 0\n",
    "for i in range(clusters.shape[0]):\n",
    "    entropy = 0\n",
    "    # Getting the label indices of the points in the cluster\n",
    "    labels = np.where(labelsInNumbers == clusters[i])\n",
    "\n",
    "    # Getting the actual labels of the points in the cluster\n",
    "    labels = y_train[labels]\n",
    "\n",
    "    # Getting the counts of each label in each cluster\n",
    "    labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    entropy = -np.sum(counts / np.sum(counts) * np.log2(counts / np.sum(counts)))\n",
    "\n",
    "    totalEntropy += entropy\n",
    "\n",
    "# Dividing by the number of clusters to get the average conditional entropy\n",
    "totalEntropy /= clusters.shape[0]\n",
    "print(\"Average conditional entropy: \", totalEntropy)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
