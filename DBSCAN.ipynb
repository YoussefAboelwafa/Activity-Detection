{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from DataMatrix import generate_data_matrix\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (7296, 45)\n",
      "y_train shape:  (7296,)\n",
      "X_test shape:  (1824, 45)\n",
      "y_test shape:  (1824,)\n"
     ]
    }
   ],
   "source": [
    "X_train1, y_train1, X_test1, y_test1 = generate_data_matrix(method=\"mean\")\n",
    "X_train2, y_train2, X_test2, y_test2 = generate_data_matrix(method=\"flatten\")\n",
    "\n",
    "n_clusters = 0\n",
    "print(\"X_train shape: \", X_train1.shape)\n",
    "print(\"y_train shape: \", y_train1.shape)\n",
    "print(\"X_test shape: \", X_test1.shape)\n",
    "print(\"y_test shape: \", y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DBSCAN(X, min_samples, eps):\n",
    "    # Initialize labels\n",
    "    labels = np.zeros(X.shape[0])\n",
    "\n",
    "    # C is the cluster counter\n",
    "    C = 0\n",
    "    for i in range(X.shape[0]):\n",
    "\n",
    "        # Skip if already labeled\n",
    "        if labels[i] != 0:\n",
    "            continue\n",
    "\n",
    "        # Find neighbors within eps\n",
    "        neighbors = np.where(np.linalg.norm(X - X[i], axis=1) <= eps)[0]\n",
    "\n",
    "        # Mark as noise\n",
    "        if len(neighbors) < min_samples:\n",
    "            labels[i] = -1\n",
    "            continue\n",
    "\n",
    "        # New cluster\n",
    "        C += 1\n",
    "\n",
    "        # Assign cluster label to point\n",
    "        labels[i] = C\n",
    "\n",
    "        # Set of points to expand\n",
    "        S = list(neighbors)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(S):\n",
    "            j = S[i]\n",
    "            if labels[j] == -1:\n",
    "                labels[j] = C\n",
    "            elif labels[j] == 0:\n",
    "                labels[j] = C\n",
    "                neighbors_j = np.where(np.linalg.norm(X - X[j], axis=1) <= eps)[0]\n",
    "                if len(neighbors_j) >= min_samples:\n",
    "                    S += list(set(neighbors_j) - set(S))\n",
    "            i += 1\n",
    "    print(\"Number of Clusters: \", C)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels Mapping Function with Ground Truth Labels with Majority Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(y_true, y_pred):\n",
    "    # Get the unique labels in y_pred\n",
    "    unique_labels = np.unique(y_pred)\n",
    "\n",
    "    # Create a dictionary to map the labels\n",
    "    label_map = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if label != -1:  # Skip noise points\n",
    "            # Find the indices of points in this cluster\n",
    "            indices = np.where(y_pred == label)[0]\n",
    "\n",
    "            # Get the actual labels of these points\n",
    "            actual_labels = y_true[indices]\n",
    "\n",
    "            # Find the most common actual label\n",
    "            majority_label = mode(actual_labels).mode\n",
    "\n",
    "            # If majority_label is an array, take the first element\n",
    "            if isinstance(majority_label, np.ndarray):\n",
    "                majority_label = majority_label[0]\n",
    "\n",
    "            # Map the predicted label to the majority label\n",
    "            label_map[label] = majority_label\n",
    "\n",
    "    # Map the labels in y_pred\n",
    "    y_pred = np.array([label_map.get(label, -1) for label in y_pred])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_score(y_true, y_pred):    \n",
    "    totalEntropy = 0\n",
    "    y_pred = y_pred.astype(int)\n",
    "    n_clusters = np.max(y_pred) + 1\n",
    "    for i in range(n_clusters):\n",
    "        entropy = 0\n",
    "        # Getting the label indices of the points in the cluster\n",
    "        labels = np.where(y_pred == i)\n",
    "\n",
    "        # Getting the actual labels of the points in the cluster\n",
    "        labels = y_true[labels]\n",
    "\n",
    "        # Getting the counts of each label in each cluster\n",
    "        labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        entropy = -np.sum(counts / np.sum(counts) * np.log2(counts / np.sum(counts)))\n",
    "\n",
    "        totalEntropy += entropy\n",
    "\n",
    "    # Dividing by the number of clusters to get the average conditional entropy\n",
    "    totalEntropy /= n_clusters\n",
    "    return totalEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 (Mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters:  152\n",
      "Number of Noise Points:  1543\n",
      "Implemented DBSCAN Evaluation Metrics:\n",
      "Precision: 87.435%\n",
      "Recall:    68.353%\n",
      "F1-Score:  71.034%\n",
      "Entropy:   0.267\n"
     ]
    }
   ],
   "source": [
    "y_pred = get_DBSCAN(X_train1, n_samples, eps)\n",
    "y_pred_mapped = map_labels(y_train1, y_pred)\n",
    "\n",
    "# Compute the accuracy\n",
    "precision = precision_score(y_train1, y_pred_mapped, average=\"weighted\")\n",
    "recall = recall_score(y_train1, y_pred_mapped, average=\"weighted\")\n",
    "f1 = f1_score(y_train1, y_pred_mapped, average=\"weighted\")\n",
    "entropy = entropy_score(y_train1, y_pred_mapped)\n",
    "\n",
    "print(\"Number of Noise Points: \", np.sum(y_pred_mapped == -1))\n",
    "print(\"Implemented DBSCAN Evaluation Metrics:\")\n",
    "print(\"Precision: {:.3f}%\".format(precision * 100))\n",
    "print(\"Recall:    {:.3f}%\".format(recall * 100))\n",
    "print(\"F1-Score:  {:.3f}%\".format(f1 * 100))\n",
    "print(\"Entropy:   {:.3f}\".format(entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 (Flatten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters:  59\n",
      "Number of Noise Points:  5871\n",
      "Implemented DBSCAN Evaluation Metrics:\n",
      "Precision: 31.579%\n",
      "Recall:    19.531%\n",
      "F1-Score:  20.861%\n"
     ]
    }
   ],
   "source": [
    "y_pred = get_DBSCAN(X_train2, n_samples, eps)\n",
    "y_pred_mapped = map_labels(y_train1, y_pred)\n",
    "\n",
    "# Compute the accuracy\n",
    "precision = precision_score(y_train2, y_pred_mapped, average=\"weighted\")\n",
    "recall = recall_score(y_train2, y_pred_mapped, average=\"weighted\")\n",
    "f1 = f1_score(y_train2, y_pred_mapped, average=\"weighted\")\n",
    "\n",
    "print(\"Number of Noise Points: \", np.sum(y_pred_mapped == -1))\n",
    "print(\"Implemented DBSCAN Evaluation Metrics:\")\n",
    "print(\"Precision: {:.3f}%\".format(precision * 100))\n",
    "print(\"Recall:    {:.3f}%\".format(recall * 100))\n",
    "print(\"F1-Score:  {:.3f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN in Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 (Mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:  152\n",
      "Number of Noise Points:  1543\n",
      "Sklearn DBSCAN Evaluation Metrics:\n",
      "Precision: 31.579%\n",
      "Recall:    68.353%\n",
      "F1-Score:  71.034%\n",
      "Entropy:   0.267\n"
     ]
    }
   ],
   "source": [
    "dbscan1 = DBSCAN(eps=eps, min_samples=n_samples)\n",
    "\n",
    "# Fit the model to the data\n",
    "dbscan1.fit(X_train1)\n",
    "\n",
    "# Print the cluster labels for each data point\n",
    "y_pred_sklearn = dbscan1.labels_\n",
    "num_clusters = len(np.unique(y_pred_sklearn[y_pred_sklearn != -1]))\n",
    "print(\"Number of clusters: \", num_clusters)\n",
    "y_pred_sklearn_mapped = map_labels(y_train1, y_pred_sklearn)\n",
    "\n",
    "percision = precision_score(y_train1, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "recall = recall_score(y_train1, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "f1 = f1_score(y_train1, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "entropy = entropy_score(y_train1, y_pred_sklearn_mapped)\n",
    "\n",
    "print(\"Number of Noise Points: \", (y_pred_sklearn == -1).sum())\n",
    "print(\"Sklearn DBSCAN Evaluation Metrics:\")\n",
    "print(\"Precision: {:.3f}%\".format(precision * 100))\n",
    "print(\"Recall:    {:.3f}%\".format(recall * 100))\n",
    "print(\"F1-Score:  {:.3f}%\".format(f1 * 100))\n",
    "print(\"Entropy:   {:.3f}\".format(entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 (Flatten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:  59\n",
      "Number of Noise Points:  5871\n",
      "Sklearn DBSCAN Evaluation Metrics:\n",
      "Precision: 31.579%\n",
      "Recall:    19.531%\n",
      "F1-Score:  20.861%\n"
     ]
    }
   ],
   "source": [
    "dbscan2 = DBSCAN(eps=eps, min_samples=n_samples)\n",
    "\n",
    "# Fit the model to the data\n",
    "dbscan2.fit(X_train2)\n",
    "\n",
    "# Print the cluster labels for each data point\n",
    "y_pred_sklearn = dbscan2.labels_\n",
    "num_clusters = len(np.unique(y_pred_sklearn[y_pred_sklearn != -1]))\n",
    "print(\"Number of clusters: \", num_clusters)\n",
    "y_pred_sklearn_mapped = map_labels(y_train2, y_pred_sklearn)\n",
    "\n",
    "percision = precision_score(y_train2, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "recall = recall_score(y_train2, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "f1 = f1_score(y_train2, y_pred_sklearn_mapped, average=\"weighted\")\n",
    "\n",
    "print(\"Number of Noise Points: \", (y_pred_sklearn == -1).sum())\n",
    "print(\"Sklearn DBSCAN Evaluation Metrics:\")\n",
    "print(\"Precision: {:.3f}%\".format(precision * 100))\n",
    "print(\"Recall:    {:.3f}%\".format(recall * 100))\n",
    "print(\"F1-Score:  {:.3f}%\".format(f1 * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
